{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MindDish.ai: Multimodal YouTube QA Bot\n",
    "\n",
    "**Ironhack Final Project - Building a Multimodal AI ChatBot for YouTube Video QA**\n",
    "\n",
    "This notebook implements a Retrieval-Augmented Generation (RAG) pipeline for cooking videos. The system retrieves YouTube video transcripts, processes them into embeddings, stores them in ChromaDB, and uses an LLM agent with 17 specialized tools to answer cooking questions.\n",
    "\n",
    "**Features:**\n",
    "- 28 curated cooking videos across 7 global cuisines\n",
    "- 17 specialized cooking tools\n",
    "- Multilingual support (Portuguese, Spanish, French, English)\n",
    "- LangSmith integration for monitoring and evaluation\n",
    "- 3-layer ingredient substitution safety system\n",
    "- Tavily web search integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install -q langchain langchain-core langchain-openai langchain-community chromadb youtube-transcript-api python-dotenv tiktoken tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Check:\n",
      "OPENAI_API_KEY: Set\n",
      "LANGCHAIN_TRACING_V2: true\n",
      "LANGCHAIN_PROJECT: MindDish_QABot\n",
      "LANGCHAIN_API_KEY: Set\n",
      "TAVILY_API_KEY: Set\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# Agent\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "# Environment setup\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Verify environment variables\n",
    "print(\"Environment Check:\")\n",
    "print(f\"OPENAI_API_KEY: {'Set' if os.getenv('OPENAI_API_KEY') else 'Missing'}\")\n",
    "print(f\"LANGCHAIN_TRACING_V2: {os.getenv('LANGCHAIN_TRACING_V2')}\")\n",
    "print(f\"LANGCHAIN_PROJECT: {os.getenv('LANGCHAIN_PROJECT')}\")\n",
    "print(f\"LANGCHAIN_API_KEY: {'Set' if os.getenv('LANGCHAIN_API_KEY') else 'Missing'}\")\n",
    "print(f\"TAVILY_API_KEY: {'Set' if os.getenv('TAVILY_API_KEY') else 'Missing'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"langsmith\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory exists: True\n",
      "Transcripts directory exists: True\n",
      "Transcript files found: 28\n"
     ]
    }
   ],
   "source": [
    "# Path configuration\n",
    "DATA_DIR = Path(\"data\")\n",
    "TRANSCRIPTS_DIR = DATA_DIR / \"transcripts\"\n",
    "CHROMA_DIR = DATA_DIR / \"chroma_minddish\"\n",
    "CURATED_INDEX_PATH = DATA_DIR / \"curated_index.json\"\n",
    "\n",
    "# Verify paths exist\n",
    "print(f\"Data directory exists: {DATA_DIR.exists()}\")\n",
    "print(f\"Transcripts directory exists: {TRANSCRIPTS_DIR.exists()}\")\n",
    "if TRANSCRIPTS_DIR.exists():\n",
    "    print(f\"Transcript files found: {len(list(TRANSCRIPTS_DIR.glob('*.txt')))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Curated Video Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curated cuisines: 7\n",
      "Total videos: 28\n",
      "\n",
      "Videos by cuisine:\n",
      "  african_cuisine: 4 videos\n",
      "  french_cuisine: 4 videos\n",
      "  portuguese_cuisine: 4 videos\n",
      "  jamaican_cuisine: 4 videos\n",
      "  syrian_cuisine: 4 videos\n",
      "  italian_cuisine: 4 videos\n",
      "  indian_cuisine: 4 videos\n"
     ]
    }
   ],
   "source": [
    "# Curated video collection - 28 videos across 7 cuisines\n",
    "curated_videos = {\n",
    "    \"african_cuisine\": [\n",
    "        {\"id\": \"vIIyn8LH1_E\", \"title\": \"Nigerian Efo Riro\", \"url\": \"https://youtu.be/vIIyn8LH1_E\"},\n",
    "        {\"id\": \"8ACc_oqhQRQ\", \"title\": \"Nigerian Moi-Moi\", \"url\": \"https://youtu.be/8ACc_oqhQRQ\"},\n",
    "        {\"id\": \"6VXYzN_gDNs\", \"title\": \"Ghanaian Kontomire Stew\", \"url\": \"https://youtu.be/6VXYzN_gDNs\"},\n",
    "        {\"id\": \"FiiTy8FpxqY\", \"title\": \"South African Chakalaka\", \"url\": \"https://youtu.be/FiiTy8FpxqY\"},\n",
    "    ],\n",
    "    \"french_cuisine\": [\n",
    "        {\"id\": \"Q5uBEWjNeTw\", \"title\": \"Flognarde Dessert\", \"url\": \"https://youtu.be/Q5uBEWjNeTw\"},\n",
    "        {\"id\": \"GFuBsSrIVaE\", \"title\": \"Coq au Vin\", \"url\": \"https://youtu.be/GFuBsSrIVaE\"},\n",
    "        {\"id\": \"tOgH_ElyGQg\", \"title\": \"Poulet au Vinaigre\", \"url\": \"https://youtu.be/tOgH_ElyGQg\"},\n",
    "        {\"id\": \"9qCO2qKrfr4\", \"title\": \"Ratatouille\", \"url\": \"https://youtu.be/9qCO2qKrfr4\"},\n",
    "    ],\n",
    "    \"portuguese_cuisine\": [\n",
    "        {\"id\": \"xuUelAOuH3o\", \"title\": \"Bacalhau\", \"url\": \"https://youtu.be/xuUelAOuH3o\"},\n",
    "        {\"id\": \"4sRwu9BnLAU\", \"title\": \"Carne Estufada\", \"url\": \"https://youtu.be/4sRwu9BnLAU\"},\n",
    "        {\"id\": \"MA4LEjxZ7io\", \"title\": \"Pastel de Nata\", \"url\": \"https://youtu.be/MA4LEjxZ7io\"},\n",
    "        {\"id\": \"2PoVTipLxoI\", \"title\": \"Bifanas\", \"url\": \"https://youtu.be/2PoVTipLxoI\"},\n",
    "    ],\n",
    "    \"jamaican_cuisine\": [\n",
    "        {\"id\": \"d6IKVNRDjUk\", \"title\": \"Jamaican Pork Shoulder\", \"url\": \"https://youtu.be/d6IKVNRDjUk\"},\n",
    "        {\"id\": \"QIG6weWWB4Q\", \"title\": \"Jamaican Curry Chicken\", \"url\": \"https://youtu.be/QIG6weWWB4Q\"},\n",
    "        {\"id\": \"RlZx52eyW4M\", \"title\": \"Sweet and Sour Fish\", \"url\": \"https://youtu.be/RlZx52eyW4M\"},\n",
    "        {\"id\": \"qHC2WBx8Cvg\", \"title\": \"Rice and Peas\", \"url\": \"https://youtu.be/qHC2WBx8Cvg\"},\n",
    "    ],\n",
    "    \"syrian_cuisine\": [\n",
    "        {\"id\": \"tB5XsB91-fQ\", \"title\": \"Shrakiye\", \"url\": \"https://youtu.be/tB5XsB91-fQ\"},\n",
    "        {\"id\": \"HMXByWj_TAY\", \"title\": \"Tabouleh\", \"url\": \"https://youtu.be/HMXByWj_TAY\"},\n",
    "        {\"id\": \"DJD4QQkItT4\", \"title\": \"Knafe Nabulsieh\", \"url\": \"https://youtu.be/DJD4QQkItT4\"},\n",
    "        {\"id\": \"xF4XRASGaC0\", \"title\": \"Fatteh\", \"url\": \"https://youtu.be/xF4XRASGaC0\"},\n",
    "    ],\n",
    "    \"italian_cuisine\": [\n",
    "        {\"id\": \"bGJMHjG85BM\", \"title\": \"Chicken Cacciatore\", \"url\": \"https://youtu.be/bGJMHjG85BM\"},\n",
    "        {\"id\": \"E--DfY3w15k\", \"title\": \"Cannelloni\", \"url\": \"https://youtu.be/E--DfY3w15k\"},\n",
    "        {\"id\": \"WttUeyXPCbU\", \"title\": \"Zozzona\", \"url\": \"https://youtu.be/WttUeyXPCbU\"},\n",
    "        {\"id\": \"llV1kYg5zNo\", \"title\": \"Gnocchi alla Sorrentina\", \"url\": \"https://youtu.be/llV1kYg5zNo\"},\n",
    "    ],\n",
    "    \"indian_cuisine\": [\n",
    "        {\"id\": \"PRw88q0NkiY\", \"title\": \"Chana Masala\", \"url\": \"https://youtu.be/PRw88q0NkiY\"},\n",
    "        {\"id\": \"nilVmkdmabs\", \"title\": \"Chilli Garlic Tawa Chicken\", \"url\": \"https://youtu.be/nilVmkdmabs\"},\n",
    "        {\"id\": \"s6h3b4tuhCE\", \"title\": \"Coconut Dosa\", \"url\": \"https://youtu.be/s6h3b4tuhCE\"},\n",
    "        {\"id\": \"wmbpOb9neLY\", \"title\": \"Garlic Naan Bread\", \"url\": \"https://youtu.be/wmbpOb9neLY\"},\n",
    "    ],\n",
    "}\n",
    "\n",
    "print(f\"Curated cuisines: {len(curated_videos)}\")\n",
    "print(f\"Total videos: {sum(len(v) for v in curated_videos.values())}\")\n",
    "print(\"\\nVideos by cuisine:\")\n",
    "for cuisine, videos in curated_videos.items():\n",
    "    print(f\"  {cuisine}: {len(videos)} videos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Transcript Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 28 entries from curated index\n",
      "Built 28 documents with transcripts\n"
     ]
    }
   ],
   "source": [
    "def clean_transcript_text(text: str) -> str:\n",
    "    \"\"\"Clean transcript text by removing timestamps, music markers, and noise.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove timestamp patterns\n",
    "    text = re.sub(r'\\d{1,2}:\\d{2}(:\\d{2})?', '', text)\n",
    "    \n",
    "    # Remove music/sound markers\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    text = re.sub(r'\\(.*?\\)', '', text)\n",
    "    \n",
    "    # Clean whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def load_curated_index(path: Path = CURATED_INDEX_PATH) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load curated_index.json.\"\"\"\n",
    "    if not path.exists():\n",
    "        logger.warning(f\"Curated index not found at {path}\")\n",
    "        return []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def build_documents_from_index(index: List[Dict[str, Any]]) -> List[Document]:\n",
    "    \"\"\"Build LangChain Documents from curated index.\"\"\"\n",
    "    docs = []\n",
    "    \n",
    "    for entry in index:\n",
    "        transcript_path = entry.get(\"transcript_path\")\n",
    "        if not transcript_path:\n",
    "            continue\n",
    "        \n",
    "        text_path = Path(transcript_path)\n",
    "        if not text_path.exists():\n",
    "            continue\n",
    "        \n",
    "        raw_text = text_path.read_text(encoding=\"utf-8\")\n",
    "        cleaned_text = clean_transcript_text(raw_text)\n",
    "        \n",
    "        if not cleaned_text:\n",
    "            continue\n",
    "        \n",
    "        metadata = {\n",
    "            \"collection\": entry.get(\"collection\", \"unknown\"),\n",
    "            \"title\": entry.get(\"title\", \"Unknown\"),\n",
    "            \"url\": entry.get(\"url\", \"\"),\n",
    "            \"video_id\": entry.get(\"video_id\", \"\"),\n",
    "            \"transcript_source\": entry.get(\"transcript_source\", \"unknown\"),\n",
    "        }\n",
    "        \n",
    "        docs.append(Document(page_content=cleaned_text, metadata=metadata))\n",
    "    \n",
    "    return docs\n",
    "\n",
    "\n",
    "# Load and build documents\n",
    "curated_index = load_curated_index()\n",
    "print(f\"Loaded {len(curated_index)} entries from curated index\")\n",
    "\n",
    "if curated_index:\n",
    "    raw_documents = build_documents_from_index(curated_index)\n",
    "    print(f\"Built {len(raw_documents)} documents with transcripts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Text Chunking and Vector Store Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 225 chunks from 28 documents\n",
      "\n",
      "Sample chunk metadata: {'collection': 'african_cuisine', 'title': 'Nigerian Efo Riro', 'url': 'https://youtu.be/vIIyn8LH1_E', 'video_id': 'vIIyn8LH1_E', 'transcript_source': 'local'}\n",
      "Sample chunk content: This is the way I've been making efro now. It comes together very quickly. It tastes unique and delicious. It's not your typical efro. If you want to see how to make it, keep watching. Don't forget to...\n"
     ]
    }
   ],
   "source": [
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    ")\n",
    "\n",
    "# Chunk documents\n",
    "if 'raw_documents' in dir() and raw_documents:\n",
    "    chunked_documents = text_splitter.split_documents(raw_documents)\n",
    "    print(f\"Created {len(chunked_documents)} chunks from {len(raw_documents)} documents\")\n",
    "    \n",
    "    if chunked_documents:\n",
    "        print(f\"\\nSample chunk metadata: {chunked_documents[0].metadata}\")\n",
    "        print(f\"Sample chunk content: {chunked_documents[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 18:44:37,900 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2025-11-29 18:44:39,216 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built and persisted vectorstore with 450 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2n/gs_200v517s5clp2y1jt0mgc0000gn/T/ipykernel_62175/2923697148.py:15: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "# Initialize embeddings and LLM\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "# Create or load ChromaDB vectorstore\n",
    "CHROMA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if 'chunked_documents' in dir() and chunked_documents:\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=chunked_documents,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=str(CHROMA_DIR),\n",
    "        collection_name=\"minddish_curated\",\n",
    "    )\n",
    "    vectorstore.persist()\n",
    "    print(f\"Built and persisted vectorstore with {vectorstore._collection.count()} chunks\")\n",
    "else:\n",
    "    vectorstore = Chroma(\n",
    "        persist_directory=str(CHROMA_DIR),\n",
    "        embedding_function=embeddings,\n",
    "        collection_name=\"minddish_curated\",\n",
    "    )\n",
    "    print(f\"Loaded existing vectorstore with {vectorstore._collection.count()} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: QA Chain and Indexed Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Chain created successfully\n"
     ]
    }
   ],
   "source": [
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 4}\n",
    ")\n",
    "\n",
    "# Format retrieved documents\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    formatted = []\n",
    "    for doc in docs:\n",
    "        source = f\"[{doc.metadata.get('collection', 'unknown')}] {doc.metadata.get('title', 'Unknown')}\"\n",
    "        formatted.append(f\"Source: {source}\\nContent: {doc.page_content}\")\n",
    "    return \"\\n\\n\".join(formatted)\n",
    "\n",
    "# QA prompt template\n",
    "qa_template = \"\"\"You are MindDish.ai, a cooking assistant that answers questions based ONLY on the provided video transcripts.\n",
    "\n",
    "Context from cooking videos:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions:\n",
    "- Answer ONLY based on the context provided\n",
    "- If the information is not in the context, say \"I don't have this information in the indexed videos\"\n",
    "- Always cite which video the information comes from\n",
    "- Be helpful and provide step-by-step instructions when relevant\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_template(qa_template)\n",
    "\n",
    "# Build QA chain (rag_chain)\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | qa_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"RAG Chain created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 28 videos\n"
     ]
    }
   ],
   "source": [
    "# Build indexed videos dictionary for tool lookups\n",
    "indexed_videos = {}\n",
    "\n",
    "if curated_index:\n",
    "    for entry in curated_index:\n",
    "        video_id = entry.get(\"video_id\")\n",
    "        if video_id:\n",
    "            indexed_videos[video_id] = {\n",
    "                \"title\": entry.get(\"title\", \"Unknown\"),\n",
    "                \"collection\": entry.get(\"collection\", \"unknown\"),\n",
    "                \"url\": entry.get(\"url\", f\"https://youtu.be/{video_id}\"),\n",
    "                \"chunks\": 0,\n",
    "                \"method\": \"youtube_transcript_api\"\n",
    "            }\n",
    "\n",
    "print(f\"Indexed {len(indexed_videos)} videos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Web Search Integration (Tavily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tavily-python in /opt/anaconda3/lib/python3.12/site-packages (0.7.13)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from tavily-python) (2.32.5)\n",
      "Requirement already satisfied: tiktoken>=0.5.1 in /opt/anaconda3/lib/python3.12/site-packages (from tavily-python) (0.12.0)\n",
      "Requirement already satisfied: httpx in /opt/anaconda3/lib/python3.12/site-packages (from tavily-python) (0.27.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken>=0.5.1->tavily-python) (2025.10.23)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->tavily-python) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->tavily-python) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->tavily-python) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->tavily-python) (2025.10.5)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.12/site-packages (from httpx->tavily-python) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx->tavily-python) (1.0.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from httpx->tavily-python) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx->tavily-python) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web search function ready\n"
     ]
    }
   ],
   "source": [
    "def web_search(query: str, max_results: int = 3, search_depth: str = \"basic\") -> dict:\n",
    "    \"\"\"Search the web using Tavily API.\"\"\"\n",
    "    try:\n",
    "        from tavily import TavilyClient\n",
    "        \n",
    "        tavily_api_key = os.getenv(\"TAVILY_API_KEY\")\n",
    "        \n",
    "        if not tavily_api_key:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"results\": [],\n",
    "                \"message\": \"TAVILY_API_KEY not found\"\n",
    "            }\n",
    "        \n",
    "        client = TavilyClient(api_key=tavily_api_key)\n",
    "        \n",
    "        response = client.search(\n",
    "            query=query,\n",
    "            max_results=max_results,\n",
    "            search_depth=search_depth,\n",
    "        )\n",
    "        \n",
    "        formatted_results = []\n",
    "        for result in response.get('results', []):\n",
    "            formatted_results.append({\n",
    "                'title': result.get('title', 'No title'),\n",
    "                'url': result.get('url', ''),\n",
    "                'content': result.get('content', ''),\n",
    "                'score': result.get('score', 0.0)\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"results\": formatted_results,\n",
    "            \"query\": query,\n",
    "            \"answer\": response.get('answer', '')\n",
    "        }\n",
    "    \n",
    "    except ImportError:\n",
    "        return {\"status\": \"error\", \"results\": [], \"message\": \"tavily-python not installed\"}\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"results\": [], \"message\": f\"Web search failed: {str(e)[:200]}\"}\n",
    "\n",
    "print(\"Web search function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Define All 17 Cooking Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core RAG tools defined (1-3)\n"
     ]
    }
   ],
   "source": [
    "# CORE RAG TOOLS (1-3)\n",
    "\n",
    "@tool\n",
    "def video_qa_tool(question: str) -> str:\n",
    "    \"\"\"Answer cooking questions using RAG across all indexed videos.\"\"\"\n",
    "    answer = rag_chain.invoke(question)\n",
    "    return answer\n",
    "\n",
    "@tool\n",
    "def transcript_search_tool(keyword: str) -> str:\n",
    "    \"\"\"Search for specific keyword mentions across all cooking videos.\"\"\"\n",
    "    results = vectorstore.similarity_search(keyword, k=5)\n",
    "    \n",
    "    video_counts = {}\n",
    "    for doc in results:\n",
    "        video_title = doc.metadata.get('title', 'Unknown')\n",
    "        video_counts[video_title] = video_counts.get(video_title, 0) + 1\n",
    "    \n",
    "    if not video_counts:\n",
    "        return f\"'{keyword}' not found in any videos\"\n",
    "    \n",
    "    response = f\"Found '{keyword}' in:\\n\"\n",
    "    for video, count in sorted(video_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        response += f\"  - {video}: {count} mention(s)\\n\"\n",
    "    \n",
    "    return response\n",
    "\n",
    "@tool\n",
    "def list_videos_tool(query: str = \"\") -> str:\n",
    "    \"\"\"List all indexed cooking videos with details.\"\"\"\n",
    "    response = f\"Indexed Cooking Videos ({len(indexed_videos)}):\\n\\n\"\n",
    "    for vid_id, info in indexed_videos.items():\n",
    "        response += f\"- {info['title']}\\n\"\n",
    "        response += f\"  ID: {vid_id} | Collection: {info['collection']}\\n\\n\"\n",
    "    return response\n",
    "\n",
    "print(\"Core RAG tools defined (1-3)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video analysis tools defined (4-6)\n"
     ]
    }
   ],
   "source": [
    "# VIDEO ANALYSIS TOOLS (4-6)\n",
    "\n",
    "@tool\n",
    "def video_summary_tool(video_title_or_id: str) -> str:\n",
    "    \"\"\"Generate a comprehensive summary of a specific cooking video.\"\"\"\n",
    "    \n",
    "    video_id = None\n",
    "    for vid, info in indexed_videos.items():\n",
    "        if video_title_or_id.lower() in info['title'].lower() or video_title_or_id == vid:\n",
    "            video_id = vid\n",
    "            break\n",
    "    \n",
    "    if not video_id:\n",
    "        return f\"Video '{video_title_or_id}' not found. Use list_videos_tool to see available videos.\"\n",
    "    \n",
    "    results = vectorstore.similarity_search(\n",
    "        \"recipe ingredients steps instructions\",\n",
    "        k=10,\n",
    "        filter={\"video_id\": video_id}\n",
    "    )\n",
    "    \n",
    "    if not results:\n",
    "        return f\"No content found for video {video_id}\"\n",
    "    \n",
    "    all_text = \" \".join([doc.page_content for doc in results[:8]])\n",
    "    summary_prompt = f\"\"\"Summarize this cooking video including:\n",
    "- Dish being made\n",
    "- Main ingredients\n",
    "- Key cooking steps\n",
    "- Cooking time and techniques\n",
    "\n",
    "Content: {all_text[:2000]}\n",
    "\n",
    "Summary:\"\"\"\n",
    "    \n",
    "    summary = llm.invoke(summary_prompt)\n",
    "    return summary.content\n",
    "\n",
    "@tool\n",
    "def compare_videos_tool(topic: str) -> str:\n",
    "    \"\"\"Compare how different cooking videos discuss a specific topic.\"\"\"\n",
    "    results = vectorstore.similarity_search(topic, k=8)\n",
    "    \n",
    "    video_content = {}\n",
    "    for doc in results:\n",
    "        video_title = doc.metadata.get('title', 'Unknown')\n",
    "        if video_title not in video_content:\n",
    "            video_content[video_title] = []\n",
    "        video_content[video_title].append(doc.page_content)\n",
    "    \n",
    "    if not video_content:\n",
    "        return f\"No videos discuss '{topic}'\"\n",
    "    \n",
    "    comparison = f\"Comparison: '{topic}'\\n\\n\"\n",
    "    for video, chunks in video_content.items():\n",
    "        excerpt = ' '.join(chunks[:2])[:200]\n",
    "        comparison += f\"{video}:\\n   {excerpt}...\\n\\n\"\n",
    "    \n",
    "    return comparison\n",
    "\n",
    "@tool\n",
    "def find_related_videos_tool(topic: str) -> str:\n",
    "    \"\"\"Find which cooking videos are most relevant to a topic.\"\"\"\n",
    "    results = vectorstore.similarity_search(topic, k=10)\n",
    "    \n",
    "    video_scores = {}\n",
    "    for doc in results:\n",
    "        video_title = doc.metadata.get('title', 'Unknown')\n",
    "        video_scores[video_title] = video_scores.get(video_title, 0) + 1\n",
    "    \n",
    "    sorted_videos = sorted(video_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    response = f\"Most relevant videos for '{topic}':\\n\\n\"\n",
    "    for i, (video, score) in enumerate(sorted_videos[:3], 1):\n",
    "        response += f\"{i}. {video} ({score} relevant segments)\\n\"\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"Video analysis tools defined (4-6)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipe-specific tools defined (7-10)\n"
     ]
    }
   ],
   "source": [
    "# RECIPE-SPECIFIC TOOLS (7-10)\n",
    "\n",
    "@tool\n",
    "def extract_ingredients_tool(video_title_or_id: str) -> str:\n",
    "    \"\"\"Extract all ingredients mentioned in a specific cooking video.\"\"\"\n",
    "    \n",
    "    video_id = None\n",
    "    for vid, info in indexed_videos.items():\n",
    "        if video_title_or_id.lower() in info['title'].lower() or video_title_or_id == vid:\n",
    "            video_id = vid\n",
    "            break\n",
    "    \n",
    "    if not video_id:\n",
    "        return f\"Video '{video_title_or_id}' not found\"\n",
    "    \n",
    "    results = vectorstore.similarity_search(\n",
    "        \"ingredients what you need shopping list\",\n",
    "        k=8,\n",
    "        filter={\"video_id\": video_id}\n",
    "    )\n",
    "    \n",
    "    if not results:\n",
    "        return \"No ingredients found\"\n",
    "    \n",
    "    all_text = \" \".join([doc.page_content for doc in results])\n",
    "    prompt = f\"\"\"Extract all ingredients mentioned in this cooking video:\n",
    "\n",
    "{all_text[:2000]}\n",
    "\n",
    "Ingredients:\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content\n",
    "\n",
    "@tool\n",
    "def cooking_time_tool(video_title_or_id: str) -> str:\n",
    "    \"\"\"Extract cooking times, prep times, and total time from a video.\"\"\"\n",
    "    \n",
    "    video_id = None\n",
    "    for vid, info in indexed_videos.items():\n",
    "        if video_title_or_id.lower() in info['title'].lower() or video_title_or_id == vid:\n",
    "            video_id = vid\n",
    "            break\n",
    "    \n",
    "    if not video_id:\n",
    "        return f\"Video '{video_title_or_id}' not found\"\n",
    "    \n",
    "    results = vectorstore.similarity_search(\n",
    "        \"minutes hours time cook bake prep\",\n",
    "        k=6,\n",
    "        filter={\"video_id\": video_id}\n",
    "    )\n",
    "    \n",
    "    if not results:\n",
    "        return \"No timing information found\"\n",
    "    \n",
    "    all_text = \" \".join([doc.page_content for doc in results])\n",
    "    prompt = f\"\"\"Extract all timing information from this cooking video:\n",
    "- Prep time\n",
    "- Cook time\n",
    "- Total time\n",
    "\n",
    "Content: {all_text[:1500]}\n",
    "\n",
    "Time breakdown:\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content\n",
    "\n",
    "@tool\n",
    "def equipment_checker_tool(video_title_or_id: str) -> str:\n",
    "    \"\"\"List all cooking equipment and tools needed for a recipe.\"\"\"\n",
    "    \n",
    "    video_id = None\n",
    "    for vid, info in indexed_videos.items():\n",
    "        if video_title_or_id.lower() in info['title'].lower() or video_title_or_id == vid:\n",
    "            video_id = vid\n",
    "            break\n",
    "    \n",
    "    if not video_id:\n",
    "        return f\"Video '{video_title_or_id}' not found\"\n",
    "    \n",
    "    results = vectorstore.similarity_search(\n",
    "        \"pan pot skillet bowl oven stove equipment tools\",\n",
    "        k=6,\n",
    "        filter={\"video_id\": video_id}\n",
    "    )\n",
    "    \n",
    "    if not results:\n",
    "        return \"No equipment information found\"\n",
    "    \n",
    "    all_text = \" \".join([doc.page_content for doc in results])\n",
    "    prompt = f\"\"\"List all cooking equipment and tools mentioned in this video:\n",
    "\n",
    "Content: {all_text[:1500]}\n",
    "\n",
    "Equipment needed:\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content\n",
    "\n",
    "@tool\n",
    "def smart_substitution_tool(original_ingredient: str, substitute: str, dish: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Suggest safe ingredient substitutions with 3-layer safety system:\n",
    "    Layer 1: Dangerous substitutions blacklist\n",
    "    Layer 2: Check indexed videos\n",
    "    Layer 3: Web search with Tavily\n",
    "    \"\"\"\n",
    "    \n",
    "    # Layer 1: Dangerous substitutions blacklist\n",
    "    dangerous_pairs = [\n",
    "        ('baking soda', 'baking powder'),\n",
    "        ('baking powder', 'baking soda'),\n",
    "        ('salt', 'sugar'),\n",
    "        ('sugar', 'salt'),\n",
    "    ]\n",
    "    \n",
    "    ingredient_lower = original_ingredient.lower()\n",
    "    substitute_lower = substitute.lower()\n",
    "    \n",
    "    for pair in dangerous_pairs:\n",
    "        if any(item in ingredient_lower for item in pair) and any(item in substitute_lower for item in pair):\n",
    "            return f\"DANGER: Substituting {original_ingredient} with {substitute} is UNSAFE!\"\n",
    "    \n",
    "    # Layer 2: Check indexed videos\n",
    "    search_query = f\"{original_ingredient} substitute {substitute} alternative\"\n",
    "    results = vectorstore.similarity_search(search_query, k=5)\n",
    "    \n",
    "    video_context = \"\"\n",
    "    for doc in results:\n",
    "        content = doc.page_content.lower()\n",
    "        if ingredient_lower in content:\n",
    "            video_context += doc.page_content + \"\\n\"\n",
    "    \n",
    "    if video_context:\n",
    "        prompt = f\"\"\"Can I use {substitute} instead of {original_ingredient} in {dish or 'this recipe'}?\n",
    "\n",
    "Video context: {video_context[:800]}\n",
    "\n",
    "Provide: YES/NO/DEPENDS, ratio, and how it affects the dish.\"\"\"\n",
    "        \n",
    "        response = llm.invoke(prompt)\n",
    "        return f\"From indexed videos:\\n{response.content}\"\n",
    "    \n",
    "    # Layer 3: Web search\n",
    "    web_result = web_search(f\"substitute {original_ingredient} with {substitute} cooking\", max_results=2)\n",
    "    \n",
    "    if web_result['status'] == 'success' and web_result['results']:\n",
    "        web_context = \"\\n\".join([r['content'][:200] for r in web_result['results']])\n",
    "        prompt = f\"\"\"Can I substitute {original_ingredient} with {substitute}?\n",
    "\n",
    "Web research: {web_context}\n",
    "\n",
    "Provide clear YES/NO, ratio, and safety notes.\"\"\"\n",
    "        \n",
    "        response = llm.invoke(prompt)\n",
    "        return f\"From web search:\\n{response.content}\"\n",
    "    \n",
    "    return f\"No reliable information found for substituting {original_ingredient} with {substitute}.\"\n",
    "\n",
    "print(\"Recipe-specific tools defined (7-10)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification & analysis tools defined (11-12)\n"
     ]
    }
   ],
   "source": [
    "# VERIFICATION & ANALYSIS TOOLS (11-12)\n",
    "\n",
    "@tool\n",
    "def recipe_fact_check_tool(claim: str) -> str:\n",
    "    \"\"\"Verify cooking claims against indexed videos and web sources.\"\"\"\n",
    "    \n",
    "    results = vectorstore.similarity_search(claim, k=4)\n",
    "    \n",
    "    if results:\n",
    "        video_evidence = \" \".join([doc.page_content for doc in results])\n",
    "        prompt = f\"\"\"Fact-check this cooking claim:\n",
    "Claim: {claim}\n",
    "\n",
    "Video evidence: {video_evidence[:1000]}\n",
    "\n",
    "Is this TRUE, FALSE, or PARTIALLY TRUE? Explain.\"\"\"\n",
    "        \n",
    "        response = llm.invoke(prompt)\n",
    "        return response.content\n",
    "    \n",
    "    return f\"No evidence found in indexed videos for: {claim}\"\n",
    "\n",
    "@tool\n",
    "def suggest_questions_tool(topic: str) -> str:\n",
    "    \"\"\"Suggest follow-up questions based on a cooking topic.\"\"\"\n",
    "    \n",
    "    results = vectorstore.similarity_search(topic, k=3)\n",
    "    \n",
    "    context = \" \".join([doc.page_content for doc in results]) if results else \"\"\n",
    "    \n",
    "    prompt = f\"\"\"Based on this cooking topic: {topic}\n",
    "\n",
    "Context: {context[:500]}\n",
    "\n",
    "Suggest 5 follow-up questions the user might want to ask:\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content\n",
    "\n",
    "print(\"Verification & analysis tools defined (11-12)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "External & utility tools defined (13-17)\n"
     ]
    }
   ],
   "source": [
    "# EXTERNAL & UTILITY TOOLS (13-17)\n",
    "\n",
    "@tool\n",
    "def web_search_cooking_tool(query: str) -> str:\n",
    "    \"\"\"Search the web for cooking information not in indexed videos.\"\"\"\n",
    "    \n",
    "    result = web_search(query + \" cooking recipe\", max_results=3)\n",
    "    \n",
    "    if result['status'] == 'success' and result['results']:\n",
    "        response = f\"Web search results for '{query}':\\n\\n\"\n",
    "        for i, r in enumerate(result['results'], 1):\n",
    "            response += f\"{i}. {r['title']}\\n   {r['content'][:150]}...\\n   URL: {r['url']}\\n\\n\"\n",
    "        return response\n",
    "    \n",
    "    return f\"No web results found for: {query}\"\n",
    "\n",
    "@tool\n",
    "def cultural_context_tool(term: str) -> str:\n",
    "    \"\"\"Explain cultural context of cooking terms, dishes, or ingredients.\"\"\"\n",
    "    \n",
    "    results = vectorstore.similarity_search(term, k=4)\n",
    "    \n",
    "    video_context = \" \".join([doc.page_content for doc in results]) if results else \"\"\n",
    "    \n",
    "    if video_context:\n",
    "        prompt = f\"\"\"Explain the cultural context of '{term}' based on this cooking video:\n",
    "\n",
    "Video Context: {video_context[:500]}\n",
    "\n",
    "Cultural Context:\"\"\"\n",
    "        \n",
    "        response = llm.invoke(prompt)\n",
    "        video_answer = f\"From videos:\\n{response.content}\\n\\n\"\n",
    "    else:\n",
    "        video_answer = f\"'{term}' not found in indexed videos.\\n\\n\"\n",
    "    \n",
    "    # Enhance with web search\n",
    "    web_result = web_search(f\"{term} traditional cooking cultural significance\", max_results=2)\n",
    "    \n",
    "    if web_result['status'] == 'success' and web_result['results']:\n",
    "        web_context = \"\\n\".join([f\"- {r['content'][:150]}\" for r in web_result['results']])\n",
    "        return video_answer + f\"Additional context from web:\\n{web_context}\"\n",
    "    \n",
    "    return video_answer\n",
    "\n",
    "@tool\n",
    "def cooking_expert_analysis_tool(question: str) -> str:\n",
    "    \"\"\"Get detailed culinary analysis combining video content + web knowledge.\"\"\"\n",
    "    \n",
    "    results = vectorstore.similarity_search(question, k=4)\n",
    "    video_context = \" \".join([doc.page_content for doc in results]) if results else \"No relevant video content.\"\n",
    "    \n",
    "    web_result = web_search(question + \" cooking science technique\", max_results=2)\n",
    "    web_context = \"\\n\".join([r['content'][:200] for r in web_result.get('results', [])[:2]]) if web_result['status'] == 'success' else \"Web unavailable.\"\n",
    "    \n",
    "    prompt = f\"\"\"As a culinary expert, analyze this question:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Video Content: {video_context[:800]}\n",
    "\n",
    "Web Research: {web_context[:400]}\n",
    "\n",
    "Provide expert analysis covering technique, science, common mistakes, and pro tips.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content\n",
    "\n",
    "@tool\n",
    "def translate_recipe_tool(text: str, target_language: str = \"Portuguese\") -> str:\n",
    "    \"\"\"Translate cooking instructions or recipes to another language.\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Translate this cooking content to {target_language}. Maintain culinary terminology accuracy.\n",
    "\n",
    "Text to translate:\n",
    "{text[:1500]}\n",
    "\n",
    "Translation in {target_language}:\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content\n",
    "\n",
    "@tool\n",
    "def nutrition_calculator_tool(video_title_or_recipe: str, servings: int = 1) -> str:\n",
    "    \"\"\"Calculate approximate nutritional information for a recipe.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        video_id = None\n",
    "        for vid, info in indexed_videos.items():\n",
    "            if video_title_or_recipe.lower() in info['title'].lower():\n",
    "                video_id = vid\n",
    "                break\n",
    "        \n",
    "        if not video_id:\n",
    "            return f\"Video '{video_title_or_recipe}' not found.\"\n",
    "        \n",
    "        ingredients_text = extract_ingredients_tool.func(video_title_or_recipe)\n",
    "        \n",
    "        web_result = web_search(f\"nutrition facts calories {video_title_or_recipe}\", max_results=2)\n",
    "        web_context = \"\\n\".join([r['content'][:200] for r in web_result.get('results', [])[:2]]) if web_result['status'] == 'success' else \"Web unavailable\"\n",
    "        \n",
    "        prompt = f\"\"\"Calculate approximate nutritional information:\n",
    "\n",
    "Recipe: {video_title_or_recipe}\n",
    "Servings: {servings}\n",
    "\n",
    "Ingredients: {ingredients_text[:1000]}\n",
    "\n",
    "Web data: {web_context[:500]}\n",
    "\n",
    "Provide: calories, protein, carbs, fat per serving. Note these are estimates.\"\"\"\n",
    "        \n",
    "        response = llm.invoke(prompt)\n",
    "        return f\"Nutritional Information: {video_title_or_recipe}\\nServings: {servings}\\n\\n{response.content}\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Could not calculate nutrition: {str(e)[:200]}\"\n",
    "\n",
    "print(\"External & utility tools defined (13-17)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Collect All Tools and Create Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MindDish.ai created with 17 specialized tools\n",
      "--------------------------------------------------\n",
      "\n",
      "Tool Categories:\n",
      "  Core RAG: video_qa, transcript_search, list_videos\n",
      "  Video Analysis: video_summary, compare_videos, find_related\n",
      "  Recipe-Specific: ingredients, cooking_time, equipment, substitution\n",
      "  Verification: fact_check, suggest_questions\n",
      "  External: web_search, cultural_context, expert_analysis, translate, nutrition\n"
     ]
    }
   ],
   "source": [
    "# Collect all 17 tools\n",
    "tools = [\n",
    "    # Core RAG tools (1-3)\n",
    "    video_qa_tool,\n",
    "    transcript_search_tool,\n",
    "    list_videos_tool,\n",
    "    # Video analysis (4-6)\n",
    "    video_summary_tool,\n",
    "    compare_videos_tool,\n",
    "    find_related_videos_tool,\n",
    "    # Recipe-specific (7-10)\n",
    "    extract_ingredients_tool,\n",
    "    cooking_time_tool,\n",
    "    equipment_checker_tool,\n",
    "    smart_substitution_tool,\n",
    "    # Verification (11-12)\n",
    "    recipe_fact_check_tool,\n",
    "    suggest_questions_tool,\n",
    "    # External & utility (13-17)\n",
    "    web_search_cooking_tool,\n",
    "    cultural_context_tool,\n",
    "    cooking_expert_analysis_tool,\n",
    "    translate_recipe_tool,\n",
    "    nutrition_calculator_tool,\n",
    "]\n",
    "\n",
    "print(f\"MindDish.ai created with {len(tools)} specialized tools\")\n",
    "print(\"-\" * 50)\n",
    "print(\"\\nTool Categories:\")\n",
    "print(\"  Core RAG: video_qa, transcript_search, list_videos\")\n",
    "print(\"  Video Analysis: video_summary, compare_videos, find_related\")\n",
    "print(\"  Recipe-Specific: ingredients, cooking_time, equipment, substitution\")\n",
    "print(\"  Verification: fact_check, suggest_questions\")\n",
    "print(\"  External: web_search, cultural_context, expert_analysis, translate, nutrition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent created with 17 tools\n"
     ]
    }
   ],
   "source": [
    "# Create agent using create_agent (LangChain 1.1.0+)\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "system_prompt = \"\"\"You are MindDish.ai, an expert cooking assistant with access to 28 curated cooking videos across 7 global cuisines (African, French, Portuguese, Jamaican, Syrian, Italian, Indian).\n",
    "\n",
    "You have 17 specialized tools at your disposal. Use the appropriate tool for each task:\n",
    "- For recipe questions: use video_qa_tool\n",
    "- To find specific mentions: use transcript_search_tool\n",
    "- To list available videos: use list_videos_tool\n",
    "- For video summaries: use video_summary_tool\n",
    "- To compare approaches: use compare_videos_tool\n",
    "- For ingredient lists: use extract_ingredients_tool\n",
    "- For cooking times: use cooking_time_tool\n",
    "- For equipment needed: use equipment_checker_tool\n",
    "- For substitutions: use smart_substitution_tool (has safety checks)\n",
    "- To verify claims: use recipe_fact_check_tool\n",
    "- For web searches: use web_search_cooking_tool\n",
    "- For cultural context: use cultural_context_tool\n",
    "- For expert analysis: use cooking_expert_analysis_tool\n",
    "- To translate: use translate_recipe_tool\n",
    "- For nutrition info: use nutrition_calculator_tool\n",
    "\n",
    "Guidelines:\n",
    "- Always cite which video your information comes from\n",
    "- If information is not in indexed videos, say so and offer to search the web\n",
    "- Be helpful, accurate, and provide step-by-step guidance when needed\"\"\"\n",
    "\n",
    "# Create the agent graph\n",
    "agent_graph = create_agent(\n",
    "    model=llm,\n",
    "    tools=tools,\n",
    "    system_prompt=system_prompt\n",
    ")\n",
    "\n",
    "print(f\"Agent created with {len(tools)} tools\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Conversation Memory and Professional Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation manager and professional handler initialized\n"
     ]
    }
   ],
   "source": [
    "class ConversationManager:\n",
    "    \"\"\"Manage conversation history with the agent.\"\"\"\n",
    "    \n",
    "    def __init__(self, agent_executor, max_history: int = 10):\n",
    "        self.agent = agent_executor\n",
    "        self.history = []\n",
    "        self.max_history = max_history\n",
    "    \n",
    "    def chat(self, user_input: str) -> str:\n",
    "        context = \"\"\n",
    "        if self.history:\n",
    "            recent = self.history[-self.max_history:]\n",
    "            context = \"\\n\".join([f\"User: {h['user']}\\nAssistant: {h['assistant'][:200]}\" for h in recent])\n",
    "            context = f\"Previous conversation:\\n{context}\\n\\n\"\n",
    "        \n",
    "        full_input = context + user_input if context else user_input\n",
    "        response = self.agent.invoke({\"input\": full_input})\n",
    "        output = response.get(\"output\", \"\")\n",
    "        \n",
    "        self.history.append({\"user\": user_input, \"assistant\": output})\n",
    "        return output\n",
    "    \n",
    "    def clear_history(self):\n",
    "        self.history = []\n",
    "\n",
    "\n",
    "class RudenessDetector:\n",
    "    \"\"\"Detect rude or inappropriate messages.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rude_words = ['stupid', 'dumb', 'idiot', 'useless', 'garbage', 'trash', 'terrible']\n",
    "    \n",
    "    def check(self, message: str) -> dict:\n",
    "        message_lower = message.lower()\n",
    "        rude_count = sum(1 for word in self.rude_words if word in message_lower)\n",
    "        is_shouting = len(message) > 10 and message.isupper()\n",
    "        \n",
    "        return {\n",
    "            \"is_rude\": rude_count > 0 or is_shouting,\n",
    "            \"severity\": \"high\" if rude_count >= 2 or is_shouting else \"medium\" if rude_count == 1 else \"none\"\n",
    "        }\n",
    "\n",
    "\n",
    "class ProfessionalHandler:\n",
    "    \"\"\"Handle messages professionally.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.detector = RudenessDetector()\n",
    "        self.responses = {\n",
    "            \"high\": \"I'm here to help with cooking questions. Let's have a constructive conversation.\",\n",
    "            \"medium\": \"I notice some frustration. What cooking question can I help you with?\"\n",
    "        }\n",
    "    \n",
    "    def process(self, message: str) -> tuple:\n",
    "        check = self.detector.check(message)\n",
    "        if check[\"is_rude\"]:\n",
    "            return True, self.responses.get(check[\"severity\"], self.responses[\"medium\"])\n",
    "        return False, message\n",
    "\n",
    "\n",
    "conversation = ConversationManager(agent_graph)\n",
    "handler = ProfessionalHandler()\n",
    "print(\"Conversation manager and professional handler initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Testing and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent Testing\n",
      "============================================================\n",
      "\n",
      "Query: What videos do you have for Portuguese cuisine?\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 18:44:41,362 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-29 18:44:43,063 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: I have the following videos for Portuguese cuisine:\n",
      "1. Bacalhau\n",
      "2. Carne Estufada\n",
      "3. Pastel de Nata\n",
      "4. Bifanas\n",
      "\n",
      "Let me know if you would like more information on any specific video!\n",
      "\n",
      "Query: How do I make Bifanas?\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 18:44:43,824 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-29 18:44:44,448 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-29 18:44:44,670 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-29 18:44:45,602 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-29 18:44:47,513 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: To make Bifanas, a Portuguese dish, you will need pork lard, onions, olive oil, and meat for the sandwiches. Here are the key steps involved in making Bifanas:\n",
      "\n",
      "1. Pound out the meat to flatten it.\n",
      "2. Make a sauce with onions and pork lard.\n",
      "3. Assemble the sandwiches with the sauce and the pounded meat.\n",
      "\n",
      "The video does not specify the exact cooking time, but the techniques involved include sauting, pounding, and assembling the sandwiches. If you need more detailed instructions or specific quant...\n",
      "\n",
      "Query: What are the ingredients in Efo Riro?\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 18:44:48,213 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-29 18:44:48,996 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-29 18:44:53,247 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: I couldn't find the specific ingredients for Efo Riro in the indexed videos. However, I found some information from a web search:\n",
      "\n",
      "Here are the ingredients typically used in Efo Riro:\n",
      "- Sliced Spinach leaves\n",
      "- Stock fish ear\n",
      "- Assorted meat\n",
      "- Sliced/ground tatashe\n",
      "- Sliced pepper\n",
      "\n",
      "You can find detailed recipes with these ingredients on websites like myactivekitchen.com, egunsifoods.com, and allnigerianfoods.com.\n",
      "\n",
      "Query: Can I substitute palm oil with vegetable oil?\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 18:44:53,877 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-29 18:44:54,142 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-29 18:44:54,827 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-29 18:44:55,605 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Yes, you can substitute palm oil with vegetable oil in your recipe. You can use vegetable oil in equal amounts as a substitute for palm oil. Keep in mind that the flavor might be slightly different, but it should still work well in the dish.\n"
     ]
    }
   ],
   "source": [
    "# Test the agent\n",
    "test_queries = [\n",
    "    \"What videos do you have for Portuguese cuisine?\",\n",
    "    \"How do I make Bifanas?\",\n",
    "    \"What are the ingredients in Efo Riro?\",\n",
    "    \"Can I substitute palm oil with vegetable oil?\",\n",
    "]\n",
    "\n",
    "print(\"Agent Testing\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    is_rude, processed = handler.process(query)\n",
    "    if is_rude:\n",
    "        print(f\"Response: {processed}\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        response = agent_graph.invoke({\"messages\": [(\"user\", query)]})\n",
    "        output = response[\"messages\"][-1].content if response.get(\"messages\") else \"No response\"\n",
    "        print(f\"Response: {output[:500]}...\" if len(output) > 500 else f\"Response: {output}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9.5: LangSmith Tracing Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith Configuration\n",
      "----------------------------------------\n",
      "LANGCHAIN_TRACING_V2: true\n",
      "LANGCHAIN_PROJECT: MindDish_QABot\n",
      "LANGCHAIN_API_KEY: Configured\n",
      "\n",
      "LangSmith tracing is ACTIVE\n",
      "View traces at: https://smith.langchain.com/\n"
     ]
    }
   ],
   "source": [
    "# Verify LangSmith is configured\n",
    "print(\"LangSmith Configuration\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"LANGCHAIN_TRACING_V2: {os.getenv('LANGCHAIN_TRACING_V2')}\")\n",
    "print(f\"LANGCHAIN_PROJECT: {os.getenv('LANGCHAIN_PROJECT')}\")\n",
    "print(f\"LANGCHAIN_API_KEY: {'Configured' if os.getenv('LANGCHAIN_API_KEY') else 'Missing'}\")\n",
    "\n",
    "if os.getenv('LANGCHAIN_TRACING_V2') == 'true' and os.getenv('LANGCHAIN_API_KEY'):\n",
    "    print(\"\\nLangSmith tracing is ACTIVE\")\n",
    "    print(\"View traces at: https://smith.langchain.com/\")\n",
    "else:\n",
    "    print(\"\\nLangSmith tracing is NOT active\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating LangSmith trace...\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 18:44:56,899 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-29 18:44:57,802 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-29 18:44:58,192 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-29 18:44:59,289 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-29 18:45:01,145 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response: To make Nigerian Efo Riro, a delicious soup, you will need spinach and a variety of spices. Here are the key steps involved in the cooking process:\n",
      "\n",
      "1. Mix all the ingredients together to create a flavorful sauce.\n",
      "2. Add in the spinach to the sauce.\n",
      "3. Cook the dish for under 30 minutes.\n",
      "\n",
      "For detailed instructions and additional tips, you can refer to the video titled \"Nigerian Efo Riro\" (ID: vIIyn8LH1_E). The recipe, along with many other Nigerian soup recipes, is available in a downloadable eb...\n"
     ]
    }
   ],
   "source": [
    "# Final test to generate LangSmith trace\n",
    "print(\"Generating LangSmith trace...\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "response = agent_graph.invoke({\"messages\": [(\"user\", \"How do I make Nigerian Efo Riro?\")]})\n",
    "output = response[\"messages\"][-1].content if response.get(\"messages\") else \"No response\"\n",
    "print(f\"\\nResponse: {output[:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implements a complete RAG-based cooking assistant with:\n",
    "\n",
    "**Core Components:**\n",
    "- YouTube transcript processing and cleaning\n",
    "- ChromaDB vector store for semantic search\n",
    "- QA chain for grounded responses\n",
    "- Agent with 17 specialized cooking tools\n",
    "- Conversation memory management\n",
    "- Professional response handling\n",
    "- Tavily web search integration\n",
    "\n",
    "**Data:**\n",
    "- 28 curated cooking videos\n",
    "- 7 global cuisines: African, French, Portuguese, Jamaican, Syrian, Italian, Indian\n",
    "\n",
    "**17 Tools:**\n",
    "1. video_qa_tool\n",
    "2. transcript_search_tool\n",
    "3. list_videos_tool\n",
    "4. video_summary_tool\n",
    "5. compare_videos_tool\n",
    "6. find_related_videos_tool\n",
    "7. extract_ingredients_tool\n",
    "8. cooking_time_tool\n",
    "9. equipment_checker_tool\n",
    "10. smart_substitution_tool\n",
    "11. recipe_fact_check_tool\n",
    "12. suggest_questions_tool\n",
    "13. web_search_cooking_tool\n",
    "14. cultural_context_tool\n",
    "15. cooking_expert_analysis_tool\n",
    "16. translate_recipe_tool\n",
    "17. nutrition_calculator_tool\n",
    "\n",
    "**Monitoring:**\n",
    "- LangSmith integration for tracing and evaluation\n",
    "\n",
    "**Deployment:**\n",
    "- Live demo: https://minddish.ai\n",
    "- API endpoint: https://api.minddish.ai"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
